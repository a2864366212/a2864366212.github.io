<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[step4-NonLinearRegression]]></title>
    <url>%2F2019%2F10%2F09%2Fstep4%2F</url>
    <content type="text"><![CDATA[Step4前向计算：输入 处理 输出误差计算：loss = predict - table反向传播|梯度下降 调整参数: 链式求导法则 9.3 双层神经网络实现非线性回归9.3.1 万能近似定理两层前馈神经网络（即一个隐层加一个输出层）和至少一层具有任何一种挤压性质的激活函数，只要隐层的神经元的数量足够，它可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的Borel可测函数。当然这个函数需要是单调递增有界的。注意，它要求的是挤压性质的激活函数，也就是类似Sigmoid的函数，如果用ReLU函数不能实现这个效果9.3.2 定义神经网络结构 输入层：一个标量X（自变量）输入层-隐藏层之间： 权重矩阵W1/B1 — 1行3列 W1= \begin{pmatrix} w^1_{11} & w^1_{12} & w^1_{13} \end{pmatrix} B1= \begin{pmatrix} b^1_{1} & b^1_{2} & b^1_{3} \end{pmatrix}隐层我们用3个神经元： Z1 = \begin{pmatrix} z^1_1 & z^1_2 & z^1_3 \end{pmatrix} A1 = \begin{pmatrix} a^1_1 & a^1_2 & a^1_3 \end{pmatrix}权重矩阵W2/B2W2的尺寸是3x1，B2的尺寸是1x1。 W2= \begin{pmatrix} w^2_{11} \\ w^2_{21} \\ w^2_{31} \end{pmatrix} B2= \begin{pmatrix} b^2_{1} \end{pmatrix}输出层由于我们只想完成一个拟合任务，所以输出层只有一个神经元：仅需得到一个因变量（Y） Z2 = \begin{pmatrix} z^2_{1} \end{pmatrix}9.3.3 前向计算隐层 线性计算 z^1_{1} = x \cdot w^1_{11} + b^1_{1} z^1_{2} = x \cdot w^1_{12} + b^1_{2} z^1_{3} = x \cdot w^1_{13} + b^1_{3} Z1 = X \cdot W1 + B1 \tag{1} 激活函数 a^1_{1} = Sigmoid(z^1_{1}) a^1_{2} = Sigmoid(z^1_{2}) a^1_{3} = Sigmoid(z^1_{3}) A1 = Sigmoid(Z1) \tag{2}输出层由于我们只想完成一个拟合任务，所以输出层只有一个神经元： z=a^1_{1}w^2_{11}+a^1_{2}w^2_{21}+a^1_{3}w^2_{31}+b^2_{1}矩阵形式： Z=A1 \cdot W2+B2 \tag{3}损失函数均方差损失函数： loss(w,b) = \frac{1}{2} (z-y)^2 \tag{4}s其中，$z$是样本预测值，$y$是样本的标签值，这里的z是第二层的输出Z。 回忆之前的简单模型，输入-&gt;单层神经元处理（线性计算+sigmoid）-&gt;输出Z1和Z 如果从局部来看，都是输出。只是Z1经过sigmoid处理后得到A再作为下一层的输入如下图|本章的神经网络|||第5章的神经网络||| 根据公式4： {\partial loss \over \partial z} = z - y \tag{5}求W2的梯度根据公式3和W2的矩阵形状： dW2={\partial loss \over \partial W2} = \begin{pmatrix} {\partial loss \over \partial z}{\partial z \over \partial w^2_{11}} \\ \\ {\partial loss \over \partial z}{\partial z \over \partial w^2_{21}} \\ \\ {\partial loss \over \partial z}{\partial z \over \partial w^2_{31}} \end{pmatrix} = \begin{pmatrix} (z-y) \cdot a^1_{1} \\ (z-y) \cdot a^1_{2} \\ (z-y) \cdot a^1_{3} \end{pmatrix} =\begin{pmatrix} a^1_{1} & a^1_{2} & a^1_{3} \end{pmatrix}^T(z-y) =A1^T(Z-Y) \tag{6}求B2的梯度 dB2={\partial loss \over \partial B2}=z-y \tag{7}|反向传播| 根据公式3和A1矩阵的形状： {\partial loss \over \partial A1} = \begin{pmatrix} {\partial loss \over \partial Z}{\partial Z \over \partial a_{11}} & {\partial loss \over \partial Z}{\partial Z \over \partial a_{12}} & {\partial loss \over \partial Z}{\partial Z \over \partial a_{13}} \end{pmatrix} = \begin{pmatrix} (z-y)w^2_{11} & (z-y)w^2_{12} & (z-y)w^2_{13} \end{pmatrix} =(z-y) \begin{pmatrix} w^2_{11} & w^2_{21} & w^2_{31} \end{pmatrix} =(z-y) \begin{pmatrix} w^2_{11} \\ w^2_{21} \\ w^2_{31} \end{pmatrix}^T=(Z-Y) \cdot W2^T \tag{8}]]></content>
      <tags>
        <tag>MachineLearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maxim_Go]]></title>
    <url>%2F2019%2F10%2F08%2FMaxim-Go%2F</url>
    <content type="text"></content>
      <tags>
        <tag>Unity</tag>
      </tags>
  </entry>
</search>
