<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[pythonLearning]]></title>
    <url>%2F2019%2F10%2F27%2FpythonLearning%2F</url>
    <content type="text"><![CDATA[为了看懂Microsoft Speech Services API的源码，学习一下python的语法python raise当程序出现错误，python会自动引发异常，也可以通过raise显示地引发异常。一旦执行了raise语句，raise后面的语句将不能执行s = Noneif s is None: raise NameErrorprint ‘is here?’ 直接抛出异常，不会执行到这里 python 继承子类直接 son(farther)class Father(object): def init(self, name): self.name=name print ( “name: %s” %( self.name) ) def getName(self): return ‘Father ‘ + self.name class Son(Father): def getName(self): return ‘Son ‘+self.name str方法如果要把一个类的实例变成 str，就需要实现特殊方法str()：例子： 不使用str()方法class Student(object): def init(self,id,name,age): self.id=id self.name=name self.age=age s=Student(111,”Bob”,18)print(s)输出结果： 使用str()方法class Student(object): def init(self,id,name,age): self.id=id self.name=name self.age=age def __str__(self): return &quot;学号:{}--姓名:{}--年龄{}&quot;.format(self.id,self.name,self.age) s=Student(111,”Bob”,18)print(s)输出结果：学号:111–姓名:Bob–年龄18 Python中的format方法类似printf()中的占位符print(“{}的性别是：{}”.format(“小明”,”男”))输出结果：小明的性别是：男]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[step4-NonLinearRegression]]></title>
    <url>%2F2019%2F10%2F09%2Fstep4%2F</url>
    <content type="text"><![CDATA[https://www.codecogs.com/latex/eqneditor.php在线LaTeX公式编辑器 Step4 转载于Microsoft/ai-edu/blob/master/B-教学案例与实践最近赶进度，就不更了，有时间会把自己做的思考与练习题放在这里。 前向计算：输入 处理 输出误差计算：loss = predict - table反向传播|梯度下降 调整参数: 链式求导法则 9.3 双层神经网络实现非线性回归9.3.1 万能近似定理两层前馈神经网络（即一个隐层加一个输出层）和至少一层具有任何一种挤压性质的激活函数，只要隐层的神经元的数量足够，它可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的Borel可测函数。当然这个函数需要是单调递增有界的。注意，它要求的是挤压性质的激活函数，也就是类似Sigmoid的函数，如果用ReLU函数不能实现这个效果9.3.2 定义神经网络结构 输入层：一个标量X（自变量）输入层-隐藏层之间： 权重矩阵W1/B1 — 1行3列 W1= \begin{pmatrix} w^1_{11} & w^1_{12} & w^1_{13} \end{pmatrix} B1= \begin{pmatrix} b^1_{1} & b^1_{2} & b^1_{3} \end{pmatrix}隐层我们用3个神经元： Z1 = \begin{pmatrix} z^1_1 & z^1_2 & z^1_3 \end{pmatrix} A1 = \begin{pmatrix} a^1_1 & a^1_2 & a^1_3 \end{pmatrix}权重矩阵W2/B2W2的尺寸是3x1，B2的尺寸是1x1。 W2= \begin{pmatrix} w^2_{11} \\ w^2_{21} \\ w^2_{31} \end{pmatrix} B2= \begin{pmatrix} b^2_{1} \end{pmatrix}输出层由于我们只想完成一个拟合任务，所以输出层只有一个神经元：仅需得到一个因变量（Y） Z2 = \begin{pmatrix} z^2_{1} \end{pmatrix}9.3.3 前向计算隐层 线性计算 z^1_{1} = x \cdot w^1_{11} + b^1_{1} z^1_{2} = x \cdot w^1_{12} + b^1_{2} z^1_{3} = x \cdot w^1_{13} + b^1_{3} Z1 = X \cdot W1 + B1 \tag{1} 激活函数 a^1_{1} = Sigmoid(z^1_{1}) a^1_{2} = Sigmoid(z^1_{2}) a^1_{3} = Sigmoid(z^1_{3}) A1 = Sigmoid(Z1) \tag{2}输出层由于我们只想完成一个拟合任务，所以输出层只有一个神经元： z=a^1_{1}w^2_{11}+a^1_{2}w^2_{21}+a^1_{3}w^2_{31}+b^2_{1}矩阵形式： Z=A1 \cdot W2+B2 \tag{3}损失函数均方差损失函数： loss(w,b) = \frac{1}{2} (z-y)^2 \tag{4}s其中，$z$是样本预测值，$y$是样本的标签值，这里的z是第二层的输出Z。 回忆之前的简单模型，输入-&gt;单层神经元处理（线性计算+sigmoid）-&gt;输出Z1和Z 如果从局部来看，都是输出。只是Z1经过sigmoid处理后得到A再作为下一层的输入如下图|本章的神经网络|||第5章的神经网络||| 根据公式4： {\partial loss \over \partial z} = z - y \tag{5}求W2的梯度根据公式3和W2的矩阵形状： dW2={\partial loss \over \partial W2} = \begin{pmatrix} {\partial loss \over \partial z}{\partial z \over \partial w^2_{11}} \\ \\ {\partial loss \over \partial z}{\partial z \over \partial w^2_{21}} \\ \\ {\partial loss \over \partial z}{\partial z \over \partial w^2_{31}} \end{pmatrix} = \begin{pmatrix} (z-y) \cdot a^1_{1} \\ (z-y) \cdot a^1_{2} \\ (z-y) \cdot a^1_{3} \end{pmatrix} =\begin{pmatrix} a^1_{1} & a^1_{2} & a^1_{3} \end{pmatrix}^T(z-y) =A1^T(Z-Y) \tag{6}求B2的梯度 dB2={\partial loss \over \partial B2}=z-y \tag{7}|反向传播| 根据公式3和A1矩阵的形状： {\partial loss \over \partial A1} = \begin{pmatrix} {\partial loss \over \partial Z}{\partial Z \over \partial a_{11}} & {\partial loss \over \partial Z}{\partial Z \over \partial a_{12}} & {\partial loss \over \partial Z}{\partial Z \over \partial a_{13}} \end{pmatrix} = \begin{pmatrix} (z-y)w^2_{11} & (z-y)w^2_{12} & (z-y)w^2_{13} \end{pmatrix} =(z-y) \begin{pmatrix} w^2_{11} & w^2_{21} & w^2_{31} \end{pmatrix} =(z-y) \begin{pmatrix} w^2_{11} \\ w^2_{21} \\ w^2_{31} \end{pmatrix}^T=(Z-Y) \cdot W2^T \tag{8}现在来看激活函数的误差传播问题，由于公式2$A1=sigmoid(Z1)$在计算时，并没有改变矩阵的形状，相当于做了一个矩阵内逐元素的的计算，所以它的导数也应该是逐元素的计算，不改变误差矩阵的形状。根据Sigmoid激活函数的导数公式，有： {\partial A1 \over \partial Z1}=A1 \odot (1-A1) => dA1 \tag{9}所以最后到达Z1的误差矩阵是： {\partial loss \over \partial Z1}={\partial loss \over \partial Z}{\partial Z \over \partial A1}{\partial A1 \over \partial Z1} =(Z-Y) \cdot W2^T \odot dA1 => dZ1 \tag{10}有了dZ1后，再向前求W1和B1的误差，就和第5章中一样了，我们直接列在下面： dW1=X^T \cdot dZ1 \tag{11} dB1=dZ1 \tag{12}在学习MachineLearning的时候，我最烦恼的事情就是矩阵相乘时候的尺寸，和是否转置。这些操作主要是为了通过矩阵相乘，来进行批量预算，故而需要适时地通过转置调整尺寸例如这里的X矩阵，为第一层的输入，它的原始形状为（特征1， 特征2， 特征3）1行-3列这是样本一的三个特征输入，而权重矩阵是3行-1列，故用于对权重矩阵进行数值调节的dW矩阵应当与权重矩阵一致，即3行-1列，此处对X矩阵转置为3行-1列再乘上一个标量z-y 对于之前学过的两输入一输出的单神经元模型有：Z=w0(即b)+w1x1+w2x2（x为特征值）A=sigmoid(Z) [如果需要的话，先不讨论] dW1=X^T \cdot dZ1 \tag{11} dB1=dZ1 \tag{12}显然成立 而本节所讲的加入隐藏层所构成的神经网络，可以看成是两个简单过程的拼接 输入 处理 输出（第一层输出作为输入）输入 处理 输出]]></content>
      <tags>
        <tag>MachineLearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maxim_Go]]></title>
    <url>%2F2019%2F10%2F08%2FMaxim-Go%2F</url>
    <content type="text"></content>
      <tags>
        <tag>Unity</tag>
      </tags>
  </entry>
</search>
