<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Gensokyo]]></title>
    <url>%2F2019%2F11%2F01%2FGensokyo%2F</url>
    <content type="text"><![CDATA[最大堆和最小堆最大堆和最小堆是二叉堆的两种形式。（完全二叉树） 最大堆：根结点的键值是所有堆结点键值中最大者，且每个结点的值都比其孩子的值大。 最小堆：根结点的键值是所有堆结点键值中最小者，且每个结点的值都比其孩子的值小。 构建方式不唯一 下面对书本上的bulid过程分析并实现：（仅仅分析最大堆，则最小堆可以重载大于号等进行构建） 关键操作：siftdown(节点下沉、下拉) 构建的算法原理基础：归纳证明假设根的两个子树都是最大堆了，设根的元素为R，则有两种情况：一、R的值大于或者等于其两个子节点，此时最大堆结构完成二、否则，R与两个子节点中大的那一个交换位置，此时最大堆结构完成，或者重复第二步直到完成 为了使得子树已经是最大堆，则可以采用递归（我们不用），或者倒着循环上来下面采用循环实现BuildHeap： 初始化curr=HeapSize/2（最后一个分支节点，开始开倒车）]]></content>
      <tags>
        <tag>Algorithm and Data Structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pythonLearning]]></title>
    <url>%2F2019%2F10%2F27%2FpythonLearning%2F</url>
    <content type="text"><![CDATA[为了看懂Microsoft Speech Services API的源码，学习一下python的语法python raise当程序出现错误，python会自动引发异常，也可以通过raise显示地引发异常。一旦执行了raise语句，raise后面的语句将不能执行 s = None if s is None: raise NameError print &#39;is here?&#39; 直接抛出异常，不会执行到这里 python 继承子类直接 son(farther) class Father(object): def __init__(self, name): self.name=name print ( &quot;name: %s&quot; %( self.name) ) def getName(self): return &#39;Father &#39; + self.name class Son(Father): def getName(self): return &#39;Son &#39;+self.name str方法如果要把一个类的实例变成 str，就需要实现特殊方法str()：例子： 不使用str()方法class Student(object): def __init__(self,id,name,age): self.id=id self.name=name self.age=age s=Student(111,”Bob”,18)print(s)输出结果： main.Student object at 0x0362EBF0 使用str()方法class Student(object): def __init__(self,id,name,age): self.id=id self.name=name self.age=age def __str__(self): return &quot;学号:{}--姓名:{}--年龄{}&quot;.format(self.id,self.name,self.age) s=Student(111,”Bob”,18)print(s)输出结果：学号:111–姓名:Bob–年龄18 Python中的format方法类似printf()中的占位符print(“{}的性别是：{}”.format(“小明”,”男”))输出结果：小明的性别是：男 Python中的self指向类实例化后的对象，自己 Python中*args**kargs的用法总结先看一下固定参数的函数两输入加法函数def sum(x, y): z = x + y return zprint(sum(1,2)) 可变位置参数：*args 列表python中规定参数前带* 的，称为可变位置参数，只是我们通常称这个可变位置参数为*args,习惯和规范而已*args：是一个列表，传入的参数会被放进列表里。（c++建工程时创建的main函数的参数似乎也有，不太记得了，好像是用来接入命令行中的输入数据的，和这个有些类似，下回想起来了再说吧） def sum(\*args): ans = 0 for i in args: ans = ans + i print(ans) sum(1, 3, 5) 计算过程：ans=1+3+5 可变关键字参数：**kwargs 键值对同理，python中规定参数前 带 ** 的，称为可变关键字参数，通常用**kwargs表示。**kwargs：是一个字典，传入的参数以键值对的形式存放到字典里。 def test(**kwargs): print(kwargs) test(a=1,b=2,c=3) 运行结果：{‘a’: 1, ‘b’: 2, ‘c’: 3}]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MicrosoftAPIReadingNotes]]></title>
    <url>%2F2019%2F10%2F27%2FMicrosoftAPIReadingNotes%2F</url>
    <content type="text"><![CDATA[azure-cognitiveservices-speech reference. 短促的字段识别（15s）start_continuous_recognition()]]></content>
      <tags>
        <tag>Microsoft Speech Services API</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[step4-NonLinearRegression]]></title>
    <url>%2F2019%2F10%2F09%2Fstep4%2F</url>
    <content type="text"><![CDATA[https://www.codecogs.com/latex/eqneditor.php在线LaTeX公式编辑器 Step4 转载于Microsoft/ai-edu/blob/master/B-教学案例与实践最近赶进度，就不更了，有时间会把自己做的思考与练习题放在这里。 前向计算：输入 处理 输出误差计算：loss = predict - table反向传播|梯度下降 调整参数: 链式求导法则 9.3 双层神经网络实现非线性回归9.3.1 万能近似定理两层前馈神经网络（即一个隐层加一个输出层）和至少一层具有任何一种挤压性质的激活函数，只要隐层的神经元的数量足够，它可以以任意的精度来近似任何从一个有限维空间到另一个有限维空间的Borel可测函数。当然这个函数需要是单调递增有界的。注意，它要求的是挤压性质的激活函数，也就是类似Sigmoid的函数，如果用ReLU函数不能实现这个效果9.3.2 定义神经网络结构 输入层：一个标量X（自变量）输入层-隐藏层之间： 权重矩阵W1/B1 — 1行3列 W1= \begin{pmatrix} w^1_{11} & w^1_{12} & w^1_{13} \end{pmatrix} B1= \begin{pmatrix} b^1_{1} & b^1_{2} & b^1_{3} \end{pmatrix}隐层我们用3个神经元： Z1 = \begin{pmatrix} z^1_1 & z^1_2 & z^1_3 \end{pmatrix} A1 = \begin{pmatrix} a^1_1 & a^1_2 & a^1_3 \end{pmatrix}权重矩阵W2/B2W2的尺寸是3x1，B2的尺寸是1x1。 W2= \begin{pmatrix} w^2_{11} \\ w^2_{21} \\ w^2_{31} \end{pmatrix} B2= \begin{pmatrix} b^2_{1} \end{pmatrix}输出层由于我们只想完成一个拟合任务，所以输出层只有一个神经元：仅需得到一个因变量（Y） Z2 = \begin{pmatrix} z^2_{1} \end{pmatrix}9.3.3 前向计算隐层 线性计算 z^1_{1} = x \cdot w^1_{11} + b^1_{1} z^1_{2} = x \cdot w^1_{12} + b^1_{2} z^1_{3} = x \cdot w^1_{13} + b^1_{3} Z1 = X \cdot W1 + B1 \tag{1} 激活函数 a^1_{1} = Sigmoid(z^1_{1}) a^1_{2} = Sigmoid(z^1_{2}) a^1_{3} = Sigmoid(z^1_{3}) A1 = Sigmoid(Z1) \tag{2}输出层由于我们只想完成一个拟合任务，所以输出层只有一个神经元： z=a^1_{1}w^2_{11}+a^1_{2}w^2_{21}+a^1_{3}w^2_{31}+b^2_{1}矩阵形式： Z=A1 \cdot W2+B2 \tag{3}损失函数均方差损失函数： loss(w,b) = \frac{1}{2} (z-y)^2 \tag{4}s其中，$z$是样本预测值，$y$是样本的标签值，这里的z是第二层的输出Z。 回忆之前的简单模型，输入-&gt;单层神经元处理（线性计算+sigmoid）-&gt;输出Z1和Z 如果从局部来看，都是输出。只是Z1经过sigmoid处理后得到A再作为下一层的输入如下图|本章的神经网络|||第5章的神经网络||| 根据公式4： {\partial loss \over \partial z} = z - y \tag{5}求W2的梯度根据公式3和W2的矩阵形状： dW2={\partial loss \over \partial W2} = \begin{pmatrix} {\partial loss \over \partial z}{\partial z \over \partial w^2_{11}} \\ \\ {\partial loss \over \partial z}{\partial z \over \partial w^2_{21}} \\ \\ {\partial loss \over \partial z}{\partial z \over \partial w^2_{31}} \end{pmatrix} = \begin{pmatrix} (z-y) \cdot a^1_{1} \\ (z-y) \cdot a^1_{2} \\ (z-y) \cdot a^1_{3} \end{pmatrix} =\begin{pmatrix} a^1_{1} & a^1_{2} & a^1_{3} \end{pmatrix}^T(z-y) =A1^T(Z-Y) \tag{6}求B2的梯度 dB2={\partial loss \over \partial B2}=z-y \tag{7}|反向传播| 根据公式3和A1矩阵的形状： {\partial loss \over \partial A1} = \begin{pmatrix} {\partial loss \over \partial Z}{\partial Z \over \partial a_{11}} & {\partial loss \over \partial Z}{\partial Z \over \partial a_{12}} & {\partial loss \over \partial Z}{\partial Z \over \partial a_{13}} \end{pmatrix} = \begin{pmatrix} (z-y)w^2_{11} & (z-y)w^2_{12} & (z-y)w^2_{13} \end{pmatrix} =(z-y) \begin{pmatrix} w^2_{11} & w^2_{21} & w^2_{31} \end{pmatrix} =(z-y) \begin{pmatrix} w^2_{11} \\ w^2_{21} \\ w^2_{31} \end{pmatrix}^T=(Z-Y) \cdot W2^T \tag{8}现在来看激活函数的误差传播问题，由于公式2$A1=sigmoid(Z1)$在计算时，并没有改变矩阵的形状，相当于做了一个矩阵内逐元素的的计算，所以它的导数也应该是逐元素的计算，不改变误差矩阵的形状。根据Sigmoid激活函数的导数公式，有： {\partial A1 \over \partial Z1}=A1 \odot (1-A1) => dA1 \tag{9}所以最后到达Z1的误差矩阵是： {\partial loss \over \partial Z1}={\partial loss \over \partial Z}{\partial Z \over \partial A1}{\partial A1 \over \partial Z1} =(Z-Y) \cdot W2^T \odot dA1 => dZ1 \tag{10}有了dZ1后，再向前求W1和B1的误差，就和第5章中一样了，我们直接列在下面： dW1=X^T \cdot dZ1 \tag{11} dB1=dZ1 \tag{12}在学习MachineLearning的时候，我最烦恼的事情就是矩阵相乘时候的尺寸，和是否转置。这些操作主要是为了通过矩阵相乘，来进行批量预算，故而需要适时地通过转置调整尺寸例如这里的X矩阵，为第一层的输入，它的原始形状为（特征1， 特征2， 特征3）1行-3列这是样本一的三个特征输入，而权重矩阵是3行-1列，故用于对权重矩阵进行数值调节的dW矩阵应当与权重矩阵一致，即3行-1列，此处对X矩阵转置为3行-1列再乘上一个标量z-y 对于之前学过的两输入一输出的单神经元模型有：Z=w0(即b)+w1x1+w2x2（x为特征值）A=sigmoid(Z) [如果需要的话，先不讨论] dW1=X^T \cdot dZ1 \tag{11} dB1=dZ1 \tag{12}显然成立 而本节所讲的加入隐藏层所构成的神经网络，可以看成是两个简单过程的拼接 输入 处理 输出（第一层输出作为输入）输入 处理 输出]]></content>
      <tags>
        <tag>MachineLearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maxim_Go]]></title>
    <url>%2F2019%2F10%2F08%2FMaxim-Go%2F</url>
    <content type="text"></content>
      <tags>
        <tag>Unity</tag>
      </tags>
  </entry>
</search>
